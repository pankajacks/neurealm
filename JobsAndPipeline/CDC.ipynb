{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa9c6c23-2717-4033-be89-4dbb54fca09a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Capturing Data Changes\n",
    "\n",
    "Capturing and replicating changes from source systems is a fundamental component of building reliable ETL pipelines. This ensures that target datasets stay up to date with source modifications. The technique used for this purpose is known as Change Data Capture (CDC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb1e840d-b511-43d4-91d1-a5fef095447c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### What is Change Data Capture (CDC)?\n",
    "**Change Data Capture (CDC)** refers to the process of identifying and capturing changes—such as inserts, updates, and deletes—in a data source and delivering them to a target system. It enables systems to remain synchronized and accurately reflect source updates.\n",
    "\n",
    "**CDC typically detects the following row-level changes:**\n",
    "\n",
    "- **Insertions** - New records added to the source and inserted into the target.\n",
    "- **Updates** - Modified records in the source that need updating in the target.\n",
    "- **Deletions** - Records removed from the source and deleted from the target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "561a4a9c-08f2-4fde-ac19-9b33d22a8532",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### CDC Feed Structure\n",
    "CDC feeds log changes at the source as events, each including affected record data and metadata such as:\n",
    "\n",
    "- **Operation Type** - Specifies whether the record was inserted, updated, or deleted.\n",
    "- **Timestamp or Version Number** - Ensures operations are applied in the correct order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f98f66a4-dde0-4718-9083-3b55aa8fba93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### CDC Data Sources\n",
    "CDC data typically originates from two types of sources:\n",
    "\n",
    "**1. Databases with Built-in CDC Features**\n",
    "\n",
    "Many modern databases support native CDC by maintaining logs that track every data change. These logs record operation types and affected rows.\n",
    "\n",
    "  - Example: Microsoft SQL Server offers native CDC, logging change operations on tables.\n",
    "  - Delta Lake also provides Change Data Feed (CDF) for tracking modifications.\n",
    "\n",
    "**2. CDC Agents**\n",
    "\n",
    "These are third-party tools that monitor databases for changes and capture before/after data along with operation types.\n",
    "\n",
    "   - Example: Debezium, an open-source CDC platform, supports databases like MySQL, PostgreSQL, SQL Server, and MongoDB. It streams change events in real time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e11e74db-153e-4a39-ad42-fe53ed161293",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### CDC Feed Delivery Methods\n",
    "CDC data can be delivered from the source in different formats:\n",
    "\n",
    "- **Streaming Feeds** - Continuous delivery of change events for near real-time synchronization.\n",
    "- **JSON Files** - Batch-captured changes written to JSON files, which are later processed to update the target.\n",
    "\n",
    "Both methods ensure timely and accurate reflection of source changes in the target system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55bf02e0-00dc-475d-9ba6-f7810d0719cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### CDC with Delta Live Tables (DLT)\n",
    "DLT provides native support for processing CDC feeds through the `APPLY CHANGES INTO` command. This command simplifies applying changes from a source feed to a target table.\n",
    "\n",
    "**Example Syntax**\n",
    "\n",
    "```\n",
    "APPLY CHANGES INTO LIVE.target_table\n",
    "FROM STREAM(LIVE.cdc_feed_table)\n",
    "KEYS (key_field)\n",
    "APPLY AS DELETE WHEN operation_field = \"DELETE\"\n",
    "SEQUENCE BY sequence_field\n",
    "COLUMNS *\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **LIVE.target_table** – The DLT table receiving the changes. Must be created beforehand.\n",
    "- **STREAM(LIVE.cdc_feed_table)** – Specifies the streaming CDC feed source.\n",
    "- **KEYS** – Defines primary key(s) to detect existing records.\n",
    "- **APPLY AS DELETE WHEN** – Specifies conditions for deletions.\n",
    "- **SEQUENCE BY** – Orders events to apply them correctly.\n",
    "- **COLUMNS** – Applies all columns; optionally, you can choose specific columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da41356e-fc39-40c0-98a7-dc2d6cb14da7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Setup Sample CDC Feed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46875eea-99a5-4347-8ddc-61c3c6d34f2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG hive_metastore;\n",
    "DROP DATABASE IF EXISTS cdc_demo CASCADE;\n",
    "CREATE DATABASE cdc_demo;\n",
    "USE cdc_demo;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1364e17a-ee23-4f5b-a60a-a0e4d7dd95cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.rm(\"dbfs:/tmp/pipeline/tables\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50ef0e08-32a0-4f7a-83d3-f290af915baa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Define schema for CDC data\n",
    "cdc_schema = StructType([\n",
    "  StructField(\"customer_id\", IntegerType(), True),\n",
    "  StructField(\"name\", StringType(), True),\n",
    "  StructField(\"email\", StringType(), True),\n",
    "  StructField(\"operation\", StringType(), True),  # INSERT, UPDATE, DELETE\n",
    "  StructField(\"version\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Sample CDC records\n",
    "cdc_data = [\n",
    "  (1, \"Alice\", \"alice@example.com\", \"INSERT\", 1),\n",
    "  (2, \"Bob\", \"bob@example.com\", \"INSERT\", 1),\n",
    "  (1, \"Alice Smith\", \"alice.smith@example.com\", \"UPDATE\", 2),\n",
    "  (2, None, None, \"DELETE\", 3),\n",
    "  (3, \"Charlie\", \"charlie@example.com\", \"INSERT\", 4),\n",
    "]\n",
    "\n",
    "# Create a static DataFrame\n",
    "df = spark.createDataFrame(cdc_data, schema=cdc_schema)\n",
    "\n",
    "# Write to Delta table to simulate a feed source\n",
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"cdc_demo.cdc_feed_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f9f4265-acde-46b4-b00b-a9ffa45cf51b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.table(\"cdc_demo.cdc_feed_table\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48c3865f-3048-4be6-8a9f-1f4824324dfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Delta Live Tables with APPLY CHANGES INTO does not hard-delete rows by default. It applies a soft delete by:\n",
    "\n",
    "- Retaining the row\n",
    "- Nullifying values\n",
    "- Marking __DeleteVersion with the version of the delete event\n",
    "\n",
    "This is done to support SCD (Slowly Changing Dimension) logic and auditability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4f7668e-50d0-42a8-9e94-49db616e19d4",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"#row_number#\":139},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1751992155343}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.table(\"customer_table\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40f5307b-ec00-4aa4-a831-dcae2b7b09be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.table(\"customer_table_mv\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d79c886-44c8-41b9-bd19-2091a6c2d14f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cdc_data = [\n",
    "    (3, \"Charles\", \"charles@example.com\", \"UPDATE\", 5),\n",
    "    (4, \"Diana\", \"diana@example.com\", \"INSERT\", 6),   \n",
    "    (2, \"Bob Reborn\", \"bob.new@example.com\", \"INSERT\", 8),\n",
    "    (3, None, None, \"DELETE\", 9),\n",
    "    (1, \"Alice Smith\", \"alice.smith@newdomain.com\", \"UPDATE\", 10),\n",
    "    (4, \"Diana\", \"diana123@example.com\", \"UPDATE\", 7)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(cdc_data, schema=cdc_schema)\n",
    "\n",
    "# Write to Delta table to simulate a feed source\n",
    "df.write.format(\"delta\").mode(\"append\").saveAsTable(\"cdc_demo.cdc_feed_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "690eb623-5457-4fc4-aec7-cc94a7d83081",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.table(\"customer_table_mv\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8546691222473192,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "CDC",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
