{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b3e4e1e-af86-4a9d-ae94-d6b9b5a7380e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Exploring Delta Live Tables (DLT)\n",
    "- DLT is a powerful tool to build production-grade data pipelines with ease.\n",
    "- Offers a simple and intuitive interface to manage and monitor pipelines.\n",
    "- Focuses on helping users extract insights, not manage infrastructure.\n",
    "- Built on Apache Spark and designed to be declarative, reliable, and scalable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f18a0f97-c74c-43c5-9422-f23e286e52d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Benefits of Delta Live Tables\n",
    "- **Simplified pipeline construction:**\n",
    "\n",
    "  Uses a declarative approach to reduce code complexity and development time.\n",
    "\n",
    "- **Automatic table dependency management:**\n",
    "\n",
    "  Utilizes Directed Acyclic Graphs (DAGs) to manage dependencies efficiently.\n",
    "\n",
    "- **Built-in data quality control:**\n",
    "\n",
    "  Supports constraints and expectations to ensure high data quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cb33784-4acb-497e-9e8b-a4ea218fe399",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### DLT vs Spark Structured Streaming\n",
    "\n",
    "| Feature                       | **Delta Live Tables (DLT)**                               | **Spark Structured Streaming**                                         |\n",
    "| ----------------------------- | --------------------------------------------------------- | ---------------------------------------------------------------------- |\n",
    "| **Framework Type**            | Declarative ETL framework built on Apache Spark           | Imperative stream processing API                                       |\n",
    "| **Pipeline Definition**       | SQL or Python with `@dlt.table` decorators                | PySpark DataFrame API using `readStream` / `writeStream`               |\n",
    "| **Checkpointing**             | Handled automatically by DLT                              | Must be configured manually via `.option(\"checkpointLocation\", \"...\")` |\n",
    "| **Data Quality Control**      | Built-in support via `EXPECT` and `CONSTRAINT` clauses    | Basic constraints via Delta Lake (e.g., NOT NULL, CHECK)               |\n",
    "| **SQL Support for Streaming** | Full SQL support (`CREATE STREAMING TABLE`)               | Cannot create streaming tables with SQL alone                          |\n",
    "| **Table Management**          | Manages table dependencies using DAGs                     | User must manage dependencies manually                                 |\n",
    "| **Error Handling**            | Declarative `ON VIOLATION` actions: DROP ROW, FAIL UPDATE | Manual handling via custom logic or exception capture                  |\n",
    "| **Observability & Logging**   | Built-in metrics and monitoring via DLT UI                | Limited by default; requires custom instrumentation                    |\n",
    "| **Development Complexity**    | Simplified with less boilerplate code                     | More complex and code-heavy                                            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c55a894-ed1a-4d10-a9a1-5ff6216fa0f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### DLT Object Types\n",
    "#### Streaming Tables\n",
    "- **Purpose**: Handle incremental data (new records only).\n",
    "- **Use Case:** Suitable for real-time or near-real-time data ingestion and processing.\n",
    "- **Behavior:**\n",
    "  - Each pipeline run processes only the new data added since the last run.\n",
    "  - Requires streaming sources, such as Auto Loader or append-only Delta tables.\n",
    "- **Processing Pattern:** Optimized for continuous updates from live data streams.\n",
    "\n",
    "  **Example syntax:**\n",
    "\n",
    "  `CREATE OR REFRESH STREAMING TABLE table_name AS`<br>\n",
    "  `SELECT * FROM <streaming source>`\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### Materialized Views (formerly known as Live Tables)\n",
    "- **Purpose**: Handle batch data (full refresh).\n",
    "- **Use Case:** Ideal when input data includes updates, deletes, or overwrites.\n",
    "- **Behavior:**\n",
    "  - On each run, the entire query is re-executed and the table/view is fully updated.\n",
    "  - Can work with both streaming and batch data sources.\n",
    "- **Processing Pattern:** Ensures the target always reflects the latest complete view of source data.\n",
    "\n",
    "  **Example syntax:**\n",
    "\n",
    "  `CREATE OR REPLACE MATERIALIZED VIEW mview AS`<br>\n",
    "  `SELECT * FROM <batch source>`\n",
    "\n",
    "---\n",
    "\n",
    "#### Live Views\n",
    "- **Purpose:** Used for temporary, intermediate transformations.\n",
    "- **Use Case:** Ideal for data quality checks, filtering, or shaping data within a pipeline, where the output doesn’t need to be stored long-term.\n",
    "- **Behavior:**\n",
    "  - Exists only during the pipeline run.\n",
    "  - Not persisted to the catalog.\n",
    "  - Used as logical layers to break down complex transformations.\n",
    "\n",
    "  **Example syntax:**\n",
    "\n",
    "  `CREATE TEMPORARY LIVE VIEW my_temp_view AS <query>`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6f68ed4-2f19-44fd-8ec4-61e6c9c3aa0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### DLT Expectations for Data Quality\n",
    "Delta Live Tables allows you to define data quality rules using expectations, which are enforced through the CONSTRAINT keyword in SQL or decorators in Python. These rules ensure that the data meets specified conditions before being processed further.\n",
    "\n",
    "**Defining Constraints**\n",
    "Use `CONSTRAINT <name> EXPECT (<condition>)` to define data rules.\n",
    "\n",
    "⚠️ **Handling Violations with `ON VIOLATION` Clause**\n",
    "\n",
    "- **DROP ROW:** Excludes rows that do not meet the condition.\n",
    "\n",
    "  `CONSTRAINT <constraint_name> EXPECT (<condition>) ON VIOLATION DROP ROW`\n",
    "\n",
    "- **FAIL UPDATE:** Fails the pipeline run if any row violates the condition.\n",
    "\n",
    "  `CONSTRAINT <constraint_name> EXPECT (<condition>)ON VIOLATION FAIL UPDATE`\n",
    "\n",
    "- **Default:** If ON VIOLATION is not specified, violations are logged but the pipeline continues processing."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DLT-intro",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
