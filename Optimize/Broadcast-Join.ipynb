{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0c1bfcf-4b71-4600-9fbf-3e6ae360dca1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Broadcast Join in Spark SQL\n",
    "\n",
    "In PySpark, the `pyspark.sql.functions.broadcast()` method is used to broadcast a smaller DataFrame, enabling efficient joins with a larger one. Since PySpark processes data across multiple nodes, joining two DataFrames typically requires a shuffleâ€”data must be exchanged between nodes to align matching keys. This is because join keys are often located on different nodes, making traditional joins costly in terms of computation and resources. Broadcasting avoids this overhead by replicating the smaller DataFrame across all nodes, allowing each node to perform a fast local (map-side) join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fdf72e2-49e9-4546-af14-8eaa1c0742f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sample employee data (larger dataset)\n",
    "employee_data = [\n",
    "    (101, \"Alice\", 1, 60000),\n",
    "    (102, \"Bob\", 2, 70000),\n",
    "    (103, \"Charlie\", 1, 75000),\n",
    "    (104, \"David\", 3, 50000),\n",
    "    (105, \"Eva\", 2, 72000)\n",
    "]\n",
    "employee_columns = [\"emp_id\", \"emp_name\", \"dept_id\", \"salary\"]\n",
    "employee_df = spark.createDataFrame(employee_data, employee_columns)\n",
    "\n",
    "# Sample department data (small lookup table)\n",
    "department_data = [\n",
    "    (1, \"HR\"),\n",
    "    (2, \"Engineering\"),\n",
    "    (3, \"Finance\")\n",
    "]\n",
    "department_columns = [\"dept_id\", \"dept_name\"]\n",
    "department_df = spark.createDataFrame(department_data, department_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48ae7c82-3f49-41d9-9adb-4f547dbbc4b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Perform a join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05212305-1ec9-4994-bcb4-91ddeadb73d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2f8984b-7122-4cc2-9889-cd91abfa9295",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "joined_df = employee_df.join(department_df, on=\"dept_id\", how=\"inner\")\n",
    "joined_df.show()\n",
    "joined_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "738bbb79-42d2-49f3-94db-1d85b03f9cfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Perform a broadcast join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3124b365-5531-4e42-9285-d785f418287a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 10*1024*1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "007e7761-acbf-4bf3-a22f-1884f79da4f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "joined_df = employee_df.join(\n",
    "    broadcast(department_df),\n",
    "    on=\"dept_id\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "joined_df.show()\n",
    "joined_df.explain()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Broadcast-Join",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
