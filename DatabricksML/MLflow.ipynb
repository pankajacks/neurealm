{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5887e85-a37b-420b-8b27-1764dd7a701c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## MLflow in Azure Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92a3b611-36f3-4a86-8ccf-7115be8be8ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### What is MLflow?\n",
    "- An open-source platform for the machine learning lifecycle\n",
    "- Commonly used as part of a machine learning operations (ML Ops) solution\n",
    "- Integrated into many commonly used Machine Learning services, including:\n",
    "  - Azure Machine Learning\n",
    "  - Microsoft Fabric\n",
    "  - Azure Databricks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "160cbbdb-bd22-452b-8dc0-595bac7b1fae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Mlflow capabilities\n",
    "\n",
    "**MLflow Tracking**\n",
    "- Log parameters, metrics, and other data when training and evaluating models\n",
    "- View and compare metrics from multiple previously run experiments\n",
    "\n",
    "**MLflow Projects**\n",
    "- Package code for consistent deployment and reproducibility\n",
    "- Define reusable Python code environments\n",
    "\n",
    "**MLflow Models**\n",
    "- Package models for distribution and deployment\n",
    "- Support for common formats (Scikit-Learn, Spark MLLib, PyTorch TensorFlow, …)\n",
    "\n",
    "**MLflow Model Registry**\n",
    "- Register and manage models, including versioning\n",
    "- Serve models for inferencing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c1b2558-39a9-43c5-87fb-dcb0ad4d1fd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Running experiments with MLflow\n",
    "MLflow experiments allow data scientists to track training runs in a collection called an experiment. Experiment runs are useful for comparing changes over time or comparing the relative performance of models with different hyperparameter values.\n",
    "\n",
    "Creating an experiment in Azure Databricks happens automatically when you start a run. In this case, the experiment's name is the name of the notebook. It's possible to export a variable named MLFLOW_EXPERIMENT_NAME to change the name of your experiment should you choose.\n",
    "\n",
    "You can use Mlflow methods to explicitly log:\n",
    "- Parameters\n",
    "- Metrics\n",
    "- Models\n",
    "\n",
    "In the Azure Databricks portal, the Experiments page enables you to view details of each experiment run; including logged values for parameters, metrics, and other artifacts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "332eff45-5970-4820-a443-9b51e0ae8e87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Registering and serving models with MLflow\n",
    "\n",
    "Model registration in MLflow helps you track models for two main reasons:\n",
    "\n",
    "- It lets you serve models easily for real-time, streaming, or batch predictions without writing extra code—MLflow creates REST APIs or batch scoring methods automatically.\n",
    "- It allows you to version models over time, so you can track changes and compare past versions.\n",
    "\n",
    "When you train a model, you can log it as part of the experiment run, including performance metrics.\n",
    "\n",
    "- You can register the model later via the experiment UI.\n",
    "- Or, you can register automatically by adding registered_model_name to log_model().\n",
    "\n",
    "Inferencing (predicting labels for new data) can be done in Azure Databricks by:\n",
    "\n",
    "- Hosting the model as a real-time REST API.\n",
    "- Performing streaming predictions on delta tables.\n",
    "- Running batch predictions and saving results to folders.\n",
    "\n",
    "You can deploy models directly from the Models page in Databricks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "538e7c4a-17e0-42a2-9aa1-ad189846b776",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Use MLflow in Azure Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "887eec4a-92d9-428a-90c4-338677c3a51f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Ingest and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "defacab5-43d5-4e73-af91-4eb1e7072785",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../PySpark/DatasetSourcePath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a27ea21f-33b3-4323-bf9c-dbfbf486d4fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "path = sourcePath + \"/dataset/penguins.csv\"   \n",
    "data = spark.read.format(\"csv\").option(\"header\", \"true\").load(path)\n",
    "data = data.dropna().select(col(\"Island\").astype(\"string\"),\n",
    "                            col(\"CulmenLength\").astype(\"float\"),\n",
    "                            col(\"CulmenDepth\").astype(\"float\"),\n",
    "                            col(\"FlipperLength\").astype(\"float\"),\n",
    "                            col(\"BodyMass\").astype(\"float\"),\n",
    "                            col(\"Species\").astype(\"int\")\n",
    "                          )\n",
    "display(data.sample(0.2))\n",
    "   \n",
    "splits = data.randomSplit([0.7, 0.3])\n",
    "train = splits[0]\n",
    "test = splits[1]\n",
    "print (\"Training Rows:\", train.count(), \" Testing Rows:\", test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "319d92d9-ba3e-48f1-85dc-facf8adb2b91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Run an MLflow experiment\n",
    "\n",
    "- MLflow lets you track model training runs and log evaluation metrics.\n",
    "- Recording these details helps you iterate and improve your model effectively.\n",
    "- You can keep using your usual libraries (like Spark MLlib) for training and evaluation.\n",
    "- Just add MLflow commands to log metrics and information while training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c063e8b8-88ee-4453-b332-0f66bdf66e26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###  Create a function\n",
    "\n",
    "- In ML projects, you often train models with different parameters and log results.\n",
    "- To make this easier, you can create a function that wraps the training process.\n",
    "- Then, simply call the function with different parameters to test and compare models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c69ab0c3-8fd3-4c02-8920-c34b71f83b99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def train_penguin_model(training_data, test_data, maxIterations, regularization):\n",
    "    import mlflow\n",
    "    import mlflow.spark\n",
    "    from pyspark.ml import Pipeline\n",
    "    from pyspark.ml.feature import StringIndexer, VectorAssembler, MinMaxScaler\n",
    "    from pyspark.ml.classification import LogisticRegression\n",
    "    from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "    import time\n",
    "   \n",
    "    # Start an MLflow run\n",
    "    with mlflow.start_run():\n",
    "   \n",
    "        catFeature = \"Island\"\n",
    "        numFeatures = [\"CulmenLength\", \"CulmenDepth\", \"FlipperLength\", \"BodyMass\"]\n",
    "   \n",
    "        # Define the feature engineering and model steps\n",
    "        catIndexer = StringIndexer(inputCol=catFeature, outputCol=catFeature + \"Idx\")\n",
    "        numVector = VectorAssembler(inputCols=numFeatures, outputCol=\"numericFeatures\")\n",
    "        numScaler = MinMaxScaler(inputCol = numVector.getOutputCol(), outputCol=\"normalizedFeatures\")\n",
    "        featureVector = VectorAssembler(inputCols=[\"IslandIdx\", \"normalizedFeatures\"], outputCol=\"Features\")\n",
    "        algo = LogisticRegression(labelCol=\"Species\", featuresCol=\"Features\", maxIter=maxIterations, regParam=regularization)\n",
    "   \n",
    "        # Chain the steps as stages in a pipeline\n",
    "        pipeline = Pipeline(stages=[catIndexer, numVector, numScaler, featureVector, algo])\n",
    "   \n",
    "        # Log training parameter values\n",
    "        print (\"Training Logistic Regression model...\")\n",
    "        mlflow.log_param('maxIter', algo.getMaxIter())\n",
    "        mlflow.log_param('regParam', algo.getRegParam())\n",
    "        model = pipeline.fit(training_data)\n",
    "   \n",
    "        # Evaluate the model and log metrics\n",
    "        prediction = model.transform(test_data)\n",
    "        metrics = [\"accuracy\", \"weightedRecall\", \"weightedPrecision\"]\n",
    "        for metric in metrics:\n",
    "            evaluator = MulticlassClassificationEvaluator(labelCol=\"Species\", predictionCol=\"prediction\", metricName=metric)\n",
    "            metricValue = evaluator.evaluate(prediction)\n",
    "            print(\"%s: %s\" % (metric, metricValue))\n",
    "            mlflow.log_metric(metric, metricValue)\n",
    "   \n",
    "   \n",
    "        # Log the model itself\n",
    "        unique_model_name = \"classifier-\" + str(time.time())\n",
    "        mlflow.spark.log_model(model, unique_model_name, mlflow.spark.get_default_conda_env())\n",
    "        modelpath = \"/model1/%s\" % (unique_model_name)\n",
    "        mlflow.spark.save_model(model, modelpath)\n",
    "   \n",
    "        print(\"Experiment run complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1f3c78b-c3c2-4d9f-93ae-fa5749971170",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_penguin_model(train, test, 10, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70766a33-6b57-46a1-b492-b05949e0ccaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Register and deploy a model with MLflow\n",
    "\n",
    "- Besides tracking training runs, MLflow helps you manage trained models.\n",
    "- Each experiment run already logs the model you trained.\n",
    "- You can also register models and deploy them so they’re ready to serve predictions to client applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "018cca26-6339-4938-8a3f-0c00abb721dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. View the details page for the most recent experiment run.\n",
    "1. Use the **Register Model** button to register the model that was logged in that experiment and when prompted, create a new model named **Penguin Predictor**.\n",
    "1. When the model has been registered, view the **Models** page (in the navigation bar on the left) and select the **Penguin Predictor** model.\n",
    "1. In the page for the **Penguin Predictor** model, use the **Use model for inference** button to create a new real-time endpoint with the following settings:\n",
    "    - **Model**: Penguin Predictor\n",
    "    - **Model version**: 1\n",
    "    - **Endpoint**: predict-penguin\n",
    "    - **Compute size**: Small\n",
    "\n",
    "    The serving endpoint is hosted in a new cluster, which it may take several minutes to create.\n",
    "  \n",
    "1. When the endpoint has been created, use the **Query endpoint** button at the top right to open an interface from which you can test the endpoint. Then in the test interface, on the **Browser** tab, enter the following JSON request and use the **Send Request** button to call the endpoint and generate a prediction.\n",
    "\n",
    "    ```json\n",
    "    {\n",
    "      \"dataframe_records\": [\n",
    "      {\n",
    "         \"Island\": \"Biscoe\",\n",
    "         \"CulmenLength\": 48.7,\n",
    "         \"CulmenDepth\": 14.1,\n",
    "         \"FlipperLength\": 210,\n",
    "         \"BodyMass\": 4450\n",
    "      }\n",
    "      ]\n",
    "    }\n",
    "    ```\n",
    "\n",
    "1. Experiment with a few different values for the penguin features and observe the results that are returned. Then, close the test interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10c0416a-4fa6-4d62-9b1e-30f1a2773757",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Delete the endpoint\n",
    "\n",
    "When the endpoint is not longer required, you should delete it to avoid unnecessary costs.\n",
    "\n",
    "In the **predict-penguin** endpoint page, in the **&#8285;** menu, select **Delete**."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "MLflow",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
